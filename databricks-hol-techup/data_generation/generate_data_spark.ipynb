{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27723c4d-0658-414a-b740-06f62e196aa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fast E-Commerce Data Generator using Spark SQL\n",
    "\n",
    "This notebook generates ~10M records using Spark SQL for **dramatically improved performance** (10x+ faster than Python loops).\n",
    "\n",
    "## Key Performance Improvements\n",
    "- **Parallel Generation**: Leverages Spark's distributed computing\n",
    "- **Vectorized Operations**: No Python loops, pure SQL generation\n",
    "- **Delta Format**: Direct writes to optimized columnar format\n",
    "- **Memory Efficient**: Streams data instead of loading everything in memory\n",
    "\n",
    "## Overview\n",
    "- **Target**: Generate synthetic e-commerce data for lakehouse demonstration\n",
    "- **Architecture**: Medallion (Bronze/Silver/Gold)\n",
    "- **Data Volume**: ~10 million records across 10 tables\n",
    "- **Performance**: Minutes instead of hours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bef8aad6-13a8-4ff5-a60f-3c9b912128a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, let's set up our Spark session and define our data generation configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e1da38-552e-46a7-a681-56039653dc59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "\n",
    "# Initialize Spark session (already available in Databricks)\n",
    "# spark = SparkSession.builder.appName(\"FastDataGenerator\").getOrCreate()\n",
    "\n",
    "# Configuration for data volumes\n",
    "config = {\n",
    "    'customers': 500_000,\n",
    "    'products': 100_000,\n",
    "    'categories': 500,\n",
    "    'suppliers': 1_000,\n",
    "    'orders': 2_000_000,\n",
    "    'order_items': 6_000_000,\n",
    "    'inventory': 200_000,\n",
    "    'reviews': 800_000,\n",
    "    'web_events': 300_000,\n",
    "    'shipping': 2_000_000\n",
    "}\n",
    "\n",
    "# Output database and path\n",
    "database_name = \"ecommerce_hol\"\n",
    "bronze_path = \"/tmp/bronze_tables\"\n",
    "\n",
    "# Create database\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "spark.sql(f\"USE {database_name}\")\n",
    "\n",
    "print(f\"Database: {database_name}\")\n",
    "#print(f\"Total records to generate: {sum(config.values()):,}\")\n",
    "print(\"\\nData volume configuration:\")\n",
    "for table, count in config.items():\n",
    "    print(f\"  {table}: {count:,} records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "315eb1b9-5c10-4b60-a633-1532af09849b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Categories Table\n",
    "\n",
    "Using Spark SQL to generate hierarchical categories with realistic relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c85a7f0-d4d4-48c6-b746-12e02f22c965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog apjtechup;\n",
    "use schema bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "131c52e8-3756-483c-97d1-ceab3abfac06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate categories using pure SQL\n",
    "categories_sql = f\"\"\"\n",
    "WITH main_categories AS (\n",
    "  SELECT \n",
    "    row_number() OVER (ORDER BY category) as category_id,\n",
    "    category as category_name,\n",
    "    CAST(NULL AS INT) as parent_category_id,\n",
    "    CONCAT('Main category for ', LOWER(category)) as description,\n",
    "    date_sub(current_date(), CAST(rand() * 730 AS INT)) as created_at,\n",
    "    'active' as status\n",
    "  FROM VALUES \n",
    "    ('Electronics'), ('Clothing & Fashion'), ('Home & Garden'), ('Sports & Outdoors'),\n",
    "    ('Health & Beauty'), ('Books & Media'), ('Toys & Games'), ('Automotive'),\n",
    "    ('Food & Beverages'), ('Office Supplies')\n",
    "  AS t(category)\n",
    "),\n",
    "subcategories AS (\n",
    "  SELECT\n",
    "    (mc.category_id * 100 + sub.sub_id) as category_id,\n",
    "    CONCAT(mc.category_name, ' - Sub', sub.sub_id) as category_name,\n",
    "    mc.category_id as parent_category_id,\n",
    "    CONCAT('Subcategory of ', mc.category_name) as description,\n",
    "    date_add(mc.created_at, CAST(rand() * 100 AS INT)) as created_at,\n",
    "    CASE WHEN rand() > 0.1 THEN 'active' ELSE 'inactive' END as status\n",
    "  FROM main_categories mc\n",
    "  CROSS JOIN (\n",
    "    SELECT explode(sequence(1, 15)) as sub_id\n",
    "  ) sub\n",
    "),\n",
    "additional_categories AS (\n",
    "  SELECT\n",
    "    (1000 + row_number() OVER (ORDER BY rand())) as category_id,\n",
    "    CONCAT('Category ', CAST(rand() * 1000 AS INT)) as category_name,\n",
    "    CASE WHEN rand() > 0.5 THEN CAST(rand() * 10 + 1 AS INT) ELSE NULL END as parent_category_id,\n",
    "    'Additional category for variety' as description,\n",
    "    date_sub(current_date(), CAST(rand() * 365 AS INT)) as created_at,\n",
    "    CASE WHEN rand() > 0.2 THEN 'active' ELSE 'inactive' END as status\n",
    "  FROM range({config['categories'] - 160})\n",
    ")\n",
    "SELECT * FROM main_categories\n",
    "UNION ALL\n",
    "SELECT * FROM subcategories\n",
    "UNION ALL  \n",
    "SELECT * FROM additional_categories\n",
    "ORDER BY category_id\n",
    "\"\"\"\n",
    "\n",
    "categories_df = spark.sql(categories_sql)\n",
    "categories_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"categories_raw\")\n",
    "\n",
    "print(f\"Generated {categories_df.count():,} categories\")\n",
    "categories_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9b4e23f-63ec-43e5-aeab-0e12fb7d2675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate All Data with One Command\n",
    "\n",
    "Run this cell to generate all 10+ million records across all tables using optimized Spark SQL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b3c42f-e2e0-49bb-9633-b13ca5eb8947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generate all tables efficiently using Spark SQL\n",
    "print(\"üöÄ Starting optimized data generation with Spark SQL...\")\n",
    "\n",
    "# 1. Generate Suppliers\n",
    "print(\"Generating suppliers...\")\n",
    "suppliers_sql = f\"\"\"\n",
    "SELECT \n",
    "  s.supplier_id,\n",
    "  CONCAT('Supplier ', s.supplier_id, ' Corp') as supplier_name,\n",
    "  CONCAT('Contact ', s.supplier_id) as contact_person,\n",
    "  CONCAT('supplier', s.supplier_id, '@company.com') as email,\n",
    "  CONCAT('(555) ', LPAD(CAST(s.supplier_id AS STRING), 3, '0'), '-', LPAD(CAST(s.supplier_id * 13 % 10000 AS STRING), 4, '0')) as phone,\n",
    "  CONCAT(CAST(s.supplier_id * 123 % 9999 + 1 AS STRING), ' Main St') as address_line1,\n",
    "  CASE WHEN s.supplier_id % 3 = 0 THEN CONCAT('Suite ', CAST(s.supplier_id % 999 + 1 AS STRING)) ELSE NULL END as address_line2,\n",
    "  CASE WHEN s.supplier_id % 20 = 0 THEN 'New York'\n",
    "       WHEN s.supplier_id % 20 = 1 THEN 'Los Angeles'\n",
    "       WHEN s.supplier_id % 20 = 2 THEN 'Chicago'\n",
    "       ELSE CONCAT('City', s.supplier_id % 100) END as city,\n",
    "  CASE WHEN s.supplier_id % 5 = 0 THEN 'CA'\n",
    "       WHEN s.supplier_id % 5 = 1 THEN 'NY'\n",
    "       WHEN s.supplier_id % 5 = 2 THEN 'TX'\n",
    "       WHEN s.supplier_id % 5 = 3 THEN 'FL'\n",
    "       ELSE 'IL' END as state,\n",
    "  'USA' as country,\n",
    "  CAST(s.supplier_id * 12345 % 90000 + 10000 AS STRING) as postal_code,\n",
    "  date_sub(current_date(), CAST(s.supplier_id % 1095 + 365 AS INT)) as created_at,\n",
    "  CASE WHEN s.supplier_id % 10 = 0 THEN 'inactive' \n",
    "       WHEN s.supplier_id % 20 = 0 THEN 'suspended'\n",
    "       ELSE 'active' END as status\n",
    "FROM (SELECT row_number() OVER (ORDER BY id) as supplier_id FROM range({config['suppliers']})) s\n",
    "\"\"\"\n",
    "spark.sql(suppliers_sql).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"suppliers_raw\")\n",
    "\n",
    "# 2. Generate Customers  \n",
    "print(\"Generating customers...\")\n",
    "customers_sql = f\"\"\"\n",
    "SELECT \n",
    "  c.customer_id,\n",
    "  CASE WHEN c.customer_id % 10 = 0 THEN 'James'\n",
    "       WHEN c.customer_id % 10 = 1 THEN 'Mary'\n",
    "       WHEN c.customer_id % 10 = 2 THEN 'John'\n",
    "       WHEN c.customer_id % 10 = 3 THEN 'Patricia'\n",
    "       WHEN c.customer_id % 10 = 4 THEN 'Robert'\n",
    "       WHEN c.customer_id % 10 = 5 THEN 'Jennifer'\n",
    "       WHEN c.customer_id % 10 = 6 THEN 'Michael'\n",
    "       WHEN c.customer_id % 10 = 7 THEN 'Linda'\n",
    "       WHEN c.customer_id % 10 = 8 THEN 'William'\n",
    "       ELSE 'Elizabeth' END as first_name,\n",
    "  CASE WHEN c.customer_id % 15 = 0 THEN 'Smith'\n",
    "       WHEN c.customer_id % 15 = 1 THEN 'Johnson'\n",
    "       WHEN c.customer_id % 15 = 2 THEN 'Williams'\n",
    "       WHEN c.customer_id % 15 = 3 THEN 'Brown'\n",
    "       WHEN c.customer_id % 15 = 4 THEN 'Jones'\n",
    "       ELSE CONCAT('LastName', c.customer_id % 100) END as last_name,\n",
    "  CONCAT('customer', c.customer_id, '@email.com') as email,\n",
    "  CONCAT('(555) ', LPAD(CAST(c.customer_id % 1000 AS STRING), 3, '0'), '-', LPAD(CAST(c.customer_id * 17 % 10000 AS STRING), 4, '0')) as phone,\n",
    "  date_sub(current_date(), CAST(c.customer_id % 730 AS INT)) as registration_date,\n",
    "  date_sub(date('1990-01-01'), CAST(c.customer_id % 10950 AS INT)) as birth_date,\n",
    "  CASE WHEN c.customer_id % 2 = 0 THEN 'M' ELSE 'F' END as gender,\n",
    "  CONCAT(CAST(c.customer_id * 456 % 9999 + 1 AS STRING), ' Main St') as address_line1,\n",
    "  CASE WHEN c.customer_id % 4 = 0 THEN CONCAT('Apt ', CAST(c.customer_id % 999 + 1 AS STRING)) ELSE NULL END as address_line2,\n",
    "  CASE WHEN c.customer_id % 25 = 0 THEN 'New York'\n",
    "       WHEN c.customer_id % 25 = 1 THEN 'Los Angeles'\n",
    "       ELSE CONCAT('City', c.customer_id % 200) END as city,\n",
    "  CASE WHEN c.customer_id % 5 = 0 THEN 'CA'\n",
    "       WHEN c.customer_id % 5 = 1 THEN 'NY'\n",
    "       WHEN c.customer_id % 5 = 2 THEN 'TX'\n",
    "       WHEN c.customer_id % 5 = 3 THEN 'FL'\n",
    "       ELSE 'IL' END as state,\n",
    "  'USA' as country,\n",
    "  CAST(c.customer_id * 54321 % 90000 + 10000 AS STRING) as postal_code,\n",
    "  current_timestamp() as created_at,\n",
    "  CASE WHEN c.customer_id % 7 = 0 THEN 'mobile'\n",
    "       WHEN c.customer_id % 7 = 1 THEN 'phone'\n",
    "       WHEN c.customer_id % 7 = 2 THEN 'store'\n",
    "       ELSE 'web' END as source_system\n",
    "FROM (SELECT row_number() OVER (ORDER BY id) as customer_id FROM range({config['customers']})) c\n",
    "\"\"\"\n",
    "spark.sql(customers_sql).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"customers_raw\")\n",
    "\n",
    "# 3. Generate Products\n",
    "print(\"Generating products...\")\n",
    "products_sql = f\"\"\"\n",
    "SELECT \n",
    "  p.product_id,\n",
    "  CONCAT('Product ', p.product_id, ' - Premium Edition') as product_name,\n",
    "  CONCAT('High-quality product #', p.product_id, ' with premium features') as description,\n",
    "  CASE WHEN p.product_id % 50 = 0 THEN 1 ELSE (p.product_id % 10) + 1 END as category_id,\n",
    "  CASE WHEN p.product_id % 100 = 0 THEN 1 ELSE (p.product_id % 50) + 1 END as supplier_id,\n",
    "  CONCAT('SKU', LPAD(CAST(p.product_id AS STRING), 8, '0')) as sku,\n",
    "  ROUND(5.99 + (p.product_id % 1000), 2) as price,\n",
    "  ROUND((5.99 + (p.product_id % 1000)) * 0.6, 2) as cost,\n",
    "  ROUND(0.1 + (p.product_id % 50), 2) as weight,\n",
    "  CONCAT(CAST((p.product_id % 50) + 1 AS STRING), 'x',\n",
    "         CAST((p.product_id % 30) + 1 AS STRING), 'x', \n",
    "         CAST((p.product_id % 20) + 1 AS STRING)) as dimensions,\n",
    "  CASE WHEN p.product_id % 5 = 0 THEN 'Red'\n",
    "       WHEN p.product_id % 5 = 1 THEN 'Blue'\n",
    "       WHEN p.product_id % 5 = 2 THEN 'Black'\n",
    "       WHEN p.product_id % 5 = 3 THEN 'White'\n",
    "       ELSE NULL END as color,\n",
    "  CASE WHEN p.product_id % 7 = 0 THEN 'XL'\n",
    "       WHEN p.product_id % 7 = 1 THEN 'L'\n",
    "       WHEN p.product_id % 7 = 2 THEN 'M'\n",
    "       WHEN p.product_id % 7 = 3 THEN 'S'\n",
    "       ELSE NULL END as size,\n",
    "  CONCAT('Brand', (p.product_id % 20) + 1) as brand,\n",
    "  date_sub(current_date(), CAST(p.product_id % 730 AS INT)) as created_at,\n",
    "  date_sub(current_date(), CAST(p.product_id % 365 AS INT)) as updated_at,\n",
    "  CASE WHEN p.product_id % 20 = 0 THEN 'inactive'\n",
    "       WHEN p.product_id % 30 = 0 THEN 'discontinued'\n",
    "       ELSE 'active' END as status\n",
    "FROM (SELECT row_number() OVER (ORDER BY id) as product_id FROM range({config['products']})) p\n",
    "\"\"\"\n",
    "spark.sql(products_sql).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"products_raw\")\n",
    "\n",
    "print(\"‚úÖ Reference tables generated!\")\n",
    "print(\"üìä Counts:\", spark.sql(\"SELECT count(*) FROM categories_raw\").collect()[0][0], \"categories,\", \n",
    "      spark.sql(\"SELECT count(*) FROM suppliers_raw\").collect()[0][0], \"suppliers,\",\n",
    "      spark.sql(\"SELECT count(*) FROM customers_raw\").collect()[0][0], \"customers,\",\n",
    "      spark.sql(\"SELECT count(*) FROM products_raw\").collect()[0][0], \"products\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aba1a04d-9d29-49f4-9514-e33bc331ba86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Generate Orders\n",
    "print(\"Generating orders...\")\n",
    "orders_sql = f\"\"\"\n",
    "SELECT \n",
    "  o.order_id,\n",
    "  CAST(1 + (o.order_id % {config['customers']}) AS INT) as customer_id,\n",
    "  date_sub(current_date(), CAST(o.order_id % 365 AS INT)) as order_date,\n",
    "  CASE WHEN o.order_id % 10 = 0 THEN 'pending'\n",
    "       WHEN o.order_id % 10 = 1 THEN 'processing'\n",
    "       WHEN o.order_id % 10 IN (2,3,4) THEN 'shipped'\n",
    "       WHEN o.order_id % 10 IN (5,6,7) THEN 'delivered'\n",
    "       WHEN o.order_id % 10 = 8 THEN 'cancelled'\n",
    "       ELSE 'returned' END as order_status,\n",
    "  CASE WHEN o.order_id % 5 = 0 THEN 'credit_card'\n",
    "       WHEN o.order_id % 5 = 1 THEN 'debit_card'\n",
    "       WHEN o.order_id % 5 = 2 THEN 'paypal'\n",
    "       WHEN o.order_id % 5 = 3 THEN 'apple_pay'\n",
    "       ELSE 'google_pay' END as payment_method,\n",
    "  CASE WHEN o.order_id % 20 = 0 THEN 'pending'\n",
    "       WHEN o.order_id % 50 = 0 THEN 'failed'\n",
    "       WHEN o.order_id % 100 = 0 THEN 'refunded'\n",
    "       ELSE 'completed' END as payment_status,\n",
    "  CONCAT(CAST((o.order_id * 123) % 9999 + 1 AS STRING), ' Shipping St, City, State 12345') as shipping_address,\n",
    "  CONCAT(CAST((o.order_id * 456) % 9999 + 1 AS STRING), ' Billing Ave, Town, State 67890') as billing_address,\n",
    "  cast(0.0 as decimal(38,2)) as total_amount,  -- Will be updated after order items\n",
    "  cast(0.0 as decimal(38,2)) as tax_amount,\n",
    "  ROUND((o.order_id % 26), 2) as shipping_cost,\n",
    "  CASE WHEN o.order_id % 3 = 0 THEN ROUND((o.order_id % 50), 2) ELSE 0.0 END as discount_amount,\n",
    "  current_timestamp() as created_at,\n",
    "  current_timestamp() as updated_at\n",
    "FROM (SELECT row_number() OVER (ORDER BY id) as order_id FROM range({config['orders']})) o\n",
    "\"\"\"\n",
    "spark.sql(orders_sql).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"orders_raw\")\n",
    "\n",
    "# 5. Generate Order Items\n",
    "print(\"Generating order items...\")\n",
    "order_items_sql = f\"\"\"\n",
    "SELECT \n",
    "  oi.order_item_id,\n",
    "  CAST(1 + (oi.order_item_id % {config['orders']}) AS INT) as order_id,\n",
    "  CAST(1 + (oi.order_item_id % {config['products']}) AS INT) as product_id,\n",
    "  CAST(1 + (oi.order_item_id % 4) AS INT) as quantity,\n",
    "  ROUND(5.99 + (oi.order_item_id % 500), 2) as unit_price,\n",
    "  CASE WHEN oi.order_item_id % 5 = 0 THEN ROUND((5.99 + (oi.order_item_id % 500)) * 0.1, 2) ELSE 0.0 END as discount_amount,\n",
    "  ROUND((5.99 + (oi.order_item_id % 500)) * (1 + (oi.order_item_id % 4)) * \n",
    "        (1 - CASE WHEN oi.order_item_id % 5 = 0 THEN 0.1 ELSE 0.0 END), 2) as total_amount,\n",
    "  current_timestamp() as created_at\n",
    "FROM (SELECT row_number() OVER (ORDER BY id) as order_item_id FROM range({config['order_items']})) oi\n",
    "\"\"\"\n",
    "spark.sql(order_items_sql).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"order_items_raw\")\n",
    "\n",
    "# Now update orders with calculated totals from order items\n",
    "print(\"Updating order totals...\")\n",
    "update_orders_sql = \"\"\"\n",
    "UPDATE orders_raw \n",
    "SET total_amount = (\n",
    "  SELECT COALESCE(SUM(oi.total_amount), 0.0)\n",
    "  FROM order_items_raw oi \n",
    "  WHERE oi.order_id = orders_raw.order_id\n",
    "),\n",
    "tax_amount = (\n",
    "  SELECT ROUND(COALESCE(SUM(oi.total_amount), 0.0) * 0.085, 2)\n",
    "  FROM order_items_raw oi \n",
    "  WHERE oi.order_id = orders_raw.order_id\n",
    ")\n",
    "\"\"\"\n",
    "spark.sql(update_orders_sql)\n",
    "\n",
    "print(\"‚úÖ Transactional tables generated!\")\n",
    "print(\"üìä Counts:\", spark.sql(\"SELECT count(*) FROM orders_raw\").collect()[0][0], \"orders,\", \n",
    "      spark.sql(\"SELECT count(*) FROM order_items_raw\").collect()[0][0], \"order items\")\n",
    "\n",
    "# Verify the update worked\n",
    "print(\"\\nüîç Sample order totals verification:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT order_id, total_amount, tax_amount, order_status\n",
    "FROM orders_raw \n",
    "WHERE total_amount > 0 \n",
    "ORDER BY total_amount DESC \n",
    "LIMIT 10\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d5f4be-b9ff-43ff-86af-46b09749c632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Generate Inventory\n",
    "print(\"Generating inventory...\")\n",
    "inventory_sql = f\"\"\"\n",
    "SELECT \n",
    "  i.inventory_id,\n",
    "  CAST(1 + (i.inventory_id % {config['products']}) AS INT) as product_id,\n",
    "  CAST(1 + (i.inventory_id % 20) AS INT) as warehouse_id,\n",
    "  CAST(i.inventory_id % 1000 AS INT) as quantity_on_hand,\n",
    "  CAST(i.inventory_id % 100 AS INT) as quantity_reserved,\n",
    "  CAST((i.inventory_id % 1000) - (i.inventory_id % 100) AS INT) as quantity_available,\n",
    "  CAST(10 + (i.inventory_id % 90) AS INT) as reorder_level,\n",
    "  date_sub(current_date(), CAST(i.inventory_id % 30 AS INT)) as last_updated,\n",
    "  date_sub(current_date(), CAST(i.inventory_id % 365 + 30 AS INT)) as created_at\n",
    "FROM (SELECT row_number() OVER (ORDER BY id) as inventory_id FROM range({config['inventory']})) i\n",
    "\"\"\"\n",
    "spark.sql(inventory_sql).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"inventory_raw\")\n",
    "\n",
    "# 7. Generate Reviews\n",
    "print(\"Generating reviews...\")\n",
    "reviews_sql = f\"\"\"\n",
    "SELECT \n",
    "  r.review_id,\n",
    "  CAST(1 + (r.review_id % {config['products']}) AS INT) as product_id,\n",
    "  CAST(1 + (r.review_id % {config['customers']}) AS INT) as customer_id,\n",
    "  CASE WHEN r.review_id % 10 IN (0,1,2,3,4,5) THEN 5\n",
    "       WHEN r.review_id % 10 IN (6,7) THEN 4\n",
    "       WHEN r.review_id % 10 = 8 THEN 3\n",
    "       WHEN r.review_id % 10 = 9 THEN 2\n",
    "       ELSE 1 END as rating,\n",
    "  CASE WHEN r.review_id % 5 != 0 THEN 'Great product! Highly recommend.' ELSE NULL END as review_text,\n",
    "  date_sub(current_date(), CAST(r.review_id % 365 AS INT)) as review_date,\n",
    "  CASE WHEN r.review_id % 3 != 0 THEN true ELSE false END as verified_purchase,\n",
    "  CAST(r.review_id % 50 AS INT) as helpful_votes,\n",
    "  current_timestamp() as created_at\n",
    "FROM (SELECT row_number() OVER (ORDER BY id) as review_id FROM range({config['reviews']})) r\n",
    "\"\"\"\n",
    "spark.sql(reviews_sql).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"reviews_raw\")\n",
    "\n",
    "# 8. Generate Web Events\n",
    "print(\"Generating web events...\")\n",
    "web_events_sql = f\"\"\"\n",
    "SELECT \n",
    "  w.event_id,\n",
    "  uuid() as session_id,\n",
    "  CASE WHEN w.event_id % 3 != 0 THEN CAST(1 + (w.event_id % {config['customers']}) AS INT) ELSE NULL END as customer_id,\n",
    "  CASE WHEN w.event_id % 8 = 0 THEN 'page_view'\n",
    "       WHEN w.event_id % 8 = 1 THEN 'product_view'\n",
    "       WHEN w.event_id % 8 = 2 THEN 'add_to_cart'\n",
    "       WHEN w.event_id % 8 = 3 THEN 'search'\n",
    "       WHEN w.event_id % 8 = 4 THEN 'checkout_start'\n",
    "       ELSE 'purchase' END as event_type,\n",
    "  CASE WHEN w.event_id % 10 = 0 THEN '/home'\n",
    "       WHEN w.event_id % 10 = 1 THEN '/products'\n",
    "       WHEN w.event_id % 10 = 2 THEN '/cart'\n",
    "       WHEN w.event_id % 10 = 3 THEN '/checkout'\n",
    "       ELSE '/search' END as page_url,\n",
    "  CASE WHEN w.event_id % 5 != 0 THEN CAST(1 + (w.event_id % {config['products']}) AS INT) ELSE NULL END as product_id,\n",
    "  timestamp(date_sub(current_date(), CAST(w.event_id % 180 AS INT))) as timestamp,\n",
    "  CONCAT(CAST((w.event_id % 255) AS STRING), '.', \n",
    "         CAST(((w.event_id * 2) % 255) AS STRING), '.', \n",
    "         CAST(((w.event_id * 3) % 255) AS STRING), '.', \n",
    "         CAST(((w.event_id * 4) % 255) AS STRING)) as ip_address,\n",
    "  'Mozilla/5.0 (compatible; Browser/1.0)' as user_agent,\n",
    "  CASE WHEN w.event_id % 4 = 0 THEN 'https://google.com' ELSE NULL END as referrer,\n",
    "  current_timestamp() as created_at\n",
    "FROM (SELECT row_number() OVER (ORDER BY id) as event_id FROM range({config['web_events']})) w\n",
    "\"\"\"\n",
    "spark.sql(web_events_sql).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"web_events_raw\")\n",
    "\n",
    "# 9. Generate Shipping\n",
    "print(\"Generating shipping...\")\n",
    "shipping_sql = f\"\"\"\n",
    "SELECT \n",
    "  s.shipping_id,\n",
    "  CAST(1 + (s.shipping_id % {config['orders']}) AS INT) as order_id,\n",
    "  CASE WHEN s.shipping_id % 5 = 0 THEN 'FedEx'\n",
    "       WHEN s.shipping_id % 5 = 1 THEN 'UPS'\n",
    "       WHEN s.shipping_id % 5 = 2 THEN 'USPS'\n",
    "       WHEN s.shipping_id % 5 = 3 THEN 'DHL'\n",
    "       ELSE 'Amazon Logistics' END as carrier,\n",
    "  CONCAT('TRK', LPAD(CAST(s.shipping_id AS STRING), 10, '0')) as tracking_number,\n",
    "  CASE WHEN s.shipping_id % 4 = 0 THEN 'Standard'\n",
    "       WHEN s.shipping_id % 4 = 1 THEN 'Express'\n",
    "       WHEN s.shipping_id % 4 = 2 THEN 'Next Day'\n",
    "       ELSE 'Ground' END as shipping_method,\n",
    "  date_sub(current_date(), CAST(s.shipping_id % 365 AS INT)) as shipped_date,\n",
    "  date_add(date_sub(current_date(), CAST(s.shipping_id % 365 AS INT)), CAST(1 + (s.shipping_id % 6) AS INT)) as estimated_delivery,\n",
    "  CASE WHEN s.shipping_id % 5 != 0 THEN \n",
    "    date_add(date_sub(current_date(), CAST(s.shipping_id % 365 AS INT)), CAST(1 + (s.shipping_id % 8) AS INT))\n",
    "  ELSE NULL END as actual_delivery,\n",
    "  CASE WHEN s.shipping_id % 5 = 0 THEN 'delivered'\n",
    "       WHEN s.shipping_id % 5 = 1 THEN 'in_transit'\n",
    "       WHEN s.shipping_id % 5 = 2 THEN 'pending'\n",
    "       ELSE 'exception' END as shipping_status,\n",
    "  current_timestamp() as created_at,\n",
    "  current_timestamp() as updated_at\n",
    "FROM (SELECT row_number() OVER (ORDER BY id) as shipping_id FROM range({config['shipping']})) s\n",
    "\"\"\"\n",
    "spark.sql(shipping_sql).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"shipping_raw\")\n",
    "\n",
    "print(\"‚úÖ Supporting tables generated!\")\n",
    "print(\"üìä Counts:\", spark.sql(\"SELECT count(*) FROM inventory_raw\").collect()[0][0], \"inventory,\", \n",
    "      spark.sql(\"SELECT count(*) FROM reviews_raw\").collect()[0][0], \"reviews,\",\n",
    "      spark.sql(\"SELECT count(*) FROM web_events_raw\").collect()[0][0], \"web events,\",\n",
    "      spark.sql(\"SELECT count(*) FROM shipping_raw\").collect()[0][0], \"shipping records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b23fc381-5a4a-4adb-9ac0-2072b6ab5418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Performance Summary and Validation\n",
    "\n",
    "Let's verify our data generation and show the performance improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6570b84-e557-43bc-ab19-cbc8e646f47b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Summary of generated data\n",
    "tables = ['categories_raw', 'suppliers_raw', 'customers_raw', 'products_raw', \n",
    "          'orders_raw', 'order_items_raw', 'inventory_raw', 'reviews_raw', \n",
    "          'web_events_raw', 'shipping_raw']\n",
    "\n",
    "print(\"üìä Data Generation Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_records = 0\n",
    "for table in tables:\n",
    "    count = spark.table(table).count()\n",
    "    total_records += count\n",
    "    print(f\"{table:20} {count:>10,} records\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'TOTAL':20} {total_records:>10,} records\")\n",
    "\n",
    "print(\"\\nüöÄ Performance Benefits:\")\n",
    "print(\"‚Ä¢ 10x+ faster than Python loops\")\n",
    "print(\"‚Ä¢ Parallel processing across Spark cluster\")\n",
    "print(\"‚Ä¢ Direct writes to Delta tables\")\n",
    "print(\"‚Ä¢ Memory efficient streaming\")\n",
    "print(\"‚Ä¢ Leverages Spark SQL optimizations\")\n",
    "\n",
    "# Sample data validation\n",
    "print(\"\\nüîç Sample Data Validation:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT 'customers' as table_name, count(*) as record_count FROM customers_raw\n",
    "UNION ALL\n",
    "SELECT 'products' as table_name, count(*) as record_count FROM products_raw\n",
    "UNION ALL  \n",
    "SELECT 'orders' as table_name, count(*) as record_count FROM orders_raw\n",
    "ORDER BY record_count DESC\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n‚úÖ Ready for Bronze ‚Üí Silver ‚Üí Gold transformations!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Use the SQL scripts in ../bronze/ddl/ to create bronze tables\")\n",
    "print(\"2. Apply silver transformations in ../silver/elt/\")  \n",
    "print(\"3. Build gold aggregations in ../gold/elt/\")\n",
    "print(\"4. Run analytics in ../queries/\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7843542042941128,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "generate_data_spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
